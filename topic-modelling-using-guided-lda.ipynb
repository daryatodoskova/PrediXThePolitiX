{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Guided LDA is a method in which the user gives some priors (words) for each or some topics, which is in a way used as a starting point by the algorithm to determine other words in the topics as needed. The package to implement this algorithm is GuidedLDA whos <a href=\"https://guidedlda.readthedocs.io/en/latest/\">user guide</a> explains how to use it on a higher level. However, recently many people across the world are facing challenges in installing the package. Thankfully, someone found a workaround for using the implementation and it is given in detail in this <a href=\"https://github.com/dex314/GuidedLDA_WorkAround\">github</a> repo. if you want to install and try running the notebook for yourself, please do ensure that you follow steps given in the above github link before starting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lda import guidedlda as glda\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting error for lda package? <hr> Getting guided lda package that I am using is not a straightforward exercise as the original packages pip install is not working. I found the workaround for implementing it <a href=\"https://github.com/dex314/GuidedLDA_WorkAround\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:11:15.140234Z",
     "start_time": "2020-06-23T06:11:15.004207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@55Bellechasse @MLP_officiel @YvesPDB Je pense...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@AmourToujours8 Il doit y avoir méprise. Je vo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@HeleneLaporteRN @MLP_officiel @RNational_off ...</td>\n",
       "      <td>quelque part</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@ericwoerth @EmmanuelMacron Non Monsieur Le To...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@dumontsoniaz Souvenez vous de Kohler, secréta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>@sebchenu @EmmanuelMacron @FaceaBaba @MLP_offi...</td>\n",
       "      <td>Gironde, Aquitaine</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>#MLP non c'est #Macron #avecvous https://t.co/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [{'text': 'MLP', 'indices': [0, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>\"Mais Macron il aura pas le choix d'essayer de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>@ThierryMichels @EmmanuelMacron @avecvousMacro...</td>\n",
       "      <td>Strasbourg, France</td>\n",
       "      <td>{'hashtags': [{'text': 'jevotemarinelepenle24a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>@LeGlaude014 @Lordglencoe6 @LFI_Forever Exacte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               text  \\\n",
       "0             0  @55Bellechasse @MLP_officiel @YvesPDB Je pense...   \n",
       "1             1  @AmourToujours8 Il doit y avoir méprise. Je vo...   \n",
       "2             2  @HeleneLaporteRN @MLP_officiel @RNational_off ...   \n",
       "3             3  @ericwoerth @EmmanuelMacron Non Monsieur Le To...   \n",
       "4             4  @dumontsoniaz Souvenez vous de Kohler, secréta...   \n",
       "..          ...                                                ...   \n",
       "995         995  @sebchenu @EmmanuelMacron @FaceaBaba @MLP_offi...   \n",
       "996         996  #MLP non c'est #Macron #avecvous https://t.co/...   \n",
       "997         997  \"Mais Macron il aura pas le choix d'essayer de...   \n",
       "998         998  @ThierryMichels @EmmanuelMacron @avecvousMacro...   \n",
       "999         999  @LeGlaude014 @Lordglencoe6 @LFI_Forever Exacte...   \n",
       "\n",
       "               location                                           entities  \n",
       "0                   NaN  {'hashtags': [], 'symbols': [], 'user_mentions...  \n",
       "1                   NaN  {'hashtags': [], 'symbols': [], 'user_mentions...  \n",
       "2          quelque part  {'hashtags': [], 'symbols': [], 'user_mentions...  \n",
       "3                   NaN  {'hashtags': [], 'symbols': [], 'user_mentions...  \n",
       "4                   NaN  {'hashtags': [], 'symbols': [], 'user_mentions...  \n",
       "..                  ...                                                ...  \n",
       "995  Gironde, Aquitaine  {'hashtags': [], 'symbols': [], 'user_mentions...  \n",
       "996                 NaN  {'hashtags': [{'text': 'MLP', 'indices': [0, 4...  \n",
       "997                 NaN  {'hashtags': [], 'symbols': [], 'user_mentions...  \n",
       "998  Strasbourg, France  {'hashtags': [{'text': 'jevotemarinelepenle24a...  \n",
       "999                 NaN  {'hashtags': [], 'symbols': [], 'user_mentions...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv(\"tweets.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:19:17.167827Z",
     "start_time": "2020-06-23T06:11:15.144960Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-bb103935f5b0>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df1['text'] =df1['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>entities</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bellechasse  MLP officiel  YvesPDB Je pense...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>bellechasse mlp officiel yvespdb pense sinc re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AmourToujours  Il doit y avoir m prise  Je vo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>amourtoujours doit avoir prise voterai bien ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>HeleneLaporteRN  MLP officiel  RNational off ...</td>\n",
       "      <td>quelque part</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>helenelaportern mlp officiel rnational chet pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ericwoerth  EmmanuelMacron Non Monsieur Le To...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>ericwoerth emmanuelmacron non monsieur tourne ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dumontsoniaz Souvenez vous de Kohler  secr ta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>dumontsoniaz souvenez kohler secr taire ral el...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  \\\n",
       "0           0     Bellechasse  MLP officiel  YvesPDB Je pense...   \n",
       "1           1   AmourToujours  Il doit y avoir m prise  Je vo...   \n",
       "2           2   HeleneLaporteRN  MLP officiel  RNational off ...   \n",
       "3           3   ericwoerth  EmmanuelMacron Non Monsieur Le To...   \n",
       "4           4   dumontsoniaz Souvenez vous de Kohler  secr ta...   \n",
       "\n",
       "       location                                           entities  \\\n",
       "0           NaN  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "1           NaN  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "2  quelque part  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "3           NaN  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "4           NaN  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  bellechasse mlp officiel yvespdb pense sinc re...  \n",
       "1  amourtoujours doit avoir prise voterai bien ma...  \n",
       "2  helenelaportern mlp officiel rnational chet pr...  \n",
       "3  ericwoerth emmanuelmacron non monsieur tourne ...  \n",
       "4  dumontsoniaz souvenez kohler secr taire ral el...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['text'] =df1['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "stopwords_list = stopwords.words('french')\n",
    "punctuations = list(set(string.punctuation))\n",
    "\n",
    "unwanted_list=punctuations+stopwords_list\n",
    "\n",
    "def clean_text_initial(text):\n",
    "    text = ' '.join([x.lower() for x in word_tokenize(text) if x.lower() not in unwanted_list and len(x)>1])\n",
    "    text = ' '.join([x.lower() for x in word_tokenize(text) if nltk.pos_tag([x])[0][1].startswith(\"NN\") or nltk.pos_tag([x])[0][1].startswith(\"JJ\")])\n",
    "    return text.strip()\n",
    "\n",
    "df1[\"clean_text\"]=df1.text.apply(lambda text:clean_text_initial(str(text)))\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating objects required for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:19:17.184962Z",
     "start_time": "2020-06-23T06:19:17.169834Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus=df1.clean_text.tolist()\n",
    "vocab=list(set(word_tokenize(\" \".join(df1.clean_text))))\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1),vocabulary=vocab)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word2id=vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:19:20.550049Z",
     "start_time": "2020-06-23T06:19:20.543281Z"
    }
   },
   "source": [
    "## Defining priors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:19:20.770932Z",
     "start_time": "2020-06-23T06:19:20.556825Z"
    }
   },
   "outputs": [],
   "source": [
    "health_words=[\"bedroom\",\"room\",\"house\",\"home\",\"airbnb\",\"condo\",\"bed\",\"blocks\",\"comfy\",\"amenities\",\"pool\",\"gym\",\n",
    "             \"cottage\",\"min\",\"minutes\", \"away\",\"duplex\",\"kitchen\",\"stay\",\"short\",\"apartment\",\"residential\",\n",
    "             \"camping\",\"distance\",\"bungalow\",\"walking\",\"neighborhood\",\"cozy\",\"cabin\",\"cabins\",\"coziness\",\n",
    "             \"families\",\"property\",\"courtyard\",\"accommodate\",\"living\", \"area\",\"minutos\"]\n",
    "\n",
    "environment_words=[\"réchauffement\", \"climatique\", \"écologique\", \"planète\"]\n",
    "\n",
    "immigration_words=[\"odometer\",\"automobile\",\"car\",\"engine\",\"automatic\", \"transmissions\",\"manual\", \"shift\",\n",
    "                      \"automotive\",\"chevrolet\",\"transmission\",\"accelerator\",\"toyota\",\"volvo\",\"nissan\",\n",
    "                     \"convertibles\",\"convertible\",\"drive\"]\n",
    "\n",
    "education_words=[\"club\",\"manager\",\"player\",\"championship\",\"contract\",\"stadium\",\"players\",\"season\",\"score\",\"scorer\",\n",
    "              \"team\",\"teammate\",\"game\",\"liverpool\",\"football\",\"nfl\",\"victory\",\"ravens\",\"boxing\",\"cricket\",\"quarterback\",\n",
    "              \"middleweight\",\"arsenal\",\"barcelona\",\"welterweight\",\"icc\",\"ipl\",\"bowlers\",\"innings\",\"bowler\",\"lightweight\",\n",
    "              \"knicks\",\"match\",\"matches\",\"soccer\",\"football\",\"playoffs\",\"premier league\",\"drs\",\"tournament\",\"fan\",\n",
    "              \"sports\",\"boxer\",\"fielders\",\"ufc\",\"linebacker\",\"coach\",\"nba\",\"referee\",\"champion\",\"injury\",\"races\",\"points\",\n",
    "              \"golf\",\"arenas\",\"pitching\",\"receiver\",\"champ\",\"cornerback\",\"mvp\",\"jayhawks\",\"quarterfinal\",\"agent\",\"ball\",\n",
    "              \"comeback\",\"shot\",\"red sox\",\"agency\",\"wins\",\"winners\",\"warriors\",\"gonzaga\",\"race\"]\n",
    "\n",
    "economy_words=[\"snapchat\",\"facebook\",\"samsung\",\"phone\",\"smartphone\",\"iphone\",\"ai\",\"uber\",\"hewlettpackard\",\"technology\",\n",
    "                ]\n",
    "\n",
    "society_words="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing prior words that are not part of vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:19:21.200585Z",
     "start_time": "2020-06-23T06:19:20.776664Z"
    }
   },
   "outputs": [],
   "source": [
    "house_words = [x for x in house_words if x in list(word2id.keys())]\n",
    "glassdoor_words = [x for x in glassdoor_words if x in list(word2id.keys())]\n",
    "automobile_words = [x for x in automobile_words if x in list(word2id.keys())]\n",
    "sports_words = [x for x in sports_words if x in list(word2id.keys())]\n",
    "tech_words = [x for x in tech_words if x in list(word2id.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating list of word lists as needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:19:21.216710Z",
     "start_time": "2020-06-23T06:19:21.205069Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_topic_list = [\n",
    "    house_words,\n",
    "    glassdoor_words,\n",
    "    automobile_words,\n",
    "    sports_words,\n",
    "    tech_words\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:19:21.364901Z",
     "start_time": "2020-06-23T06:19:21.220383Z"
    }
   },
   "outputs": [],
   "source": [
    "model = glda.GuidedLDA(n_topics=5, n_iter=2000, random_state=7, refresh=20,alpha=0.01,eta=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting priors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:19:21.465894Z",
     "start_time": "2020-06-23T06:19:21.375437Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:20:25.636968Z",
     "start_time": "2020-06-23T06:19:21.473501Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the model output topics and top 10 words per topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:20:25.809538Z",
     "start_time": "2020-06-23T06:20:25.640932Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging the topics to create id - topic file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:20:25.821536Z",
     "start_time": "2020-06-23T06:20:25.814439Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_num_name = {\"Topic 0\":\"room_rentals\",\n",
    "                  \"Topic 1\":\"glassdoor_reviews\",\n",
    "                  \"Topic 2\":\"Automobiles\",\n",
    "                  \"Topic 3\":\"sports_news\",\n",
    "                  \"Topic 4\":\"tech_news\"}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:20:25.971354Z",
     "start_time": "2020-06-23T06:20:25.828575Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_topics(model_glda,X,num_topics,dataframe,col_name):\n",
    "    \"\"\"\n",
    "    A function which creates dataframe with documents, their dominant topic, along with their probabilities\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    model_glda - Guided LDA trained model\n",
    "    X - Document term frequency table\n",
    "    num_topics - Number of topics the model was trained for\n",
    "    dataframe - Dataframe consisting of cleaned text column\n",
    "    col_name - Column name in dataframe holding cleaned text\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    A dataframe with document number, topic, probability of topic\n",
    "    \"\"\"\n",
    "    df_doc_top = pd.DataFrame()\n",
    "    final_list = []\n",
    "    for index in range(len(dataframe[col_name])):\n",
    "        word_id_dict = dict((x,y) for x,y in zip([x for x in range(num_topics)],np.round(model.transform(X[index])*100,1).tolist()[0]))\n",
    "        word_score_list = []\n",
    "        for index in range(num_topics):\n",
    "            try:\n",
    "                value = word_id_dict[index]\n",
    "            except:\n",
    "                value = 0\n",
    "            word_score_list.append(value)\n",
    "        final_list.append(word_score_list)\n",
    "\n",
    "    df_doc_top = pd.DataFrame(final_list)\n",
    "    df_doc_top.columns = ['Topic ' + str(i) for i in range(num_topics)]\n",
    "    df_doc_top.index = ['Document ' + str(i) for i in range(len(dataframe[col_name]))]\n",
    "\n",
    "    df_doc_top[\"Dominant_Topic\"] = df_doc_top.idxmax(axis=1).tolist()\n",
    "    df_doc_top[\"Topic_Probability\"] = df_doc_top.max(axis=1).tolist()\n",
    "    document_df = df_doc_top.reset_index().rename(columns={\"index\":\"Document\"})[[\"Document\",\"Dominant_Topic\",\"Topic_Probability\"]]\n",
    "\n",
    "    return document_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:21:21.409359Z",
     "start_time": "2020-06-23T06:20:25.971354Z"
    }
   },
   "outputs": [],
   "source": [
    "document_df=get_doc_topics(model,X,5,df1,\"clean_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:21:21.424961Z",
     "start_time": "2020-06-23T06:21:21.414395Z"
    }
   },
   "outputs": [],
   "source": [
    "submission=pd.concat([df1.Id,document_df.Dominant_Topic],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:21:21.582776Z",
     "start_time": "2020-06-23T06:21:21.430413Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.Dominant_Topic=submission.Dominant_Topic.replace(topic_num_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:21:21.725149Z",
     "start_time": "2020-06-23T06:21:21.588070Z"
    }
   },
   "outputs": [],
   "source": [
    "submission=submission.set_index(\"Id\").rename(columns={\"Dominant_Topic\":\"topic\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:21:21.850767Z",
     "start_time": "2020-06-23T06:21:21.729792Z"
    }
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
